{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the function to get latitude and longitude given an address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lat_lng_json(address,country='ALL'):\n",
    "    address = '_'.join(address.split(' '))\n",
    "    url=\"https://maps.googleapis.com/maps/api/geocode/json?address=%s\" % (address)\n",
    "    import requests\n",
    "    response = requests.get(url)\n",
    "    i=0\n",
    "    if response.status_code == 200:\n",
    "        coordinate = []\n",
    "        if country=='ALL':\n",
    "            for x in response.json()['results']:\n",
    "                coordinate.append(x['geometry']['location']['lat'])\n",
    "                coordinate.append(x['geometry']['location']['lng'])\n",
    "        else:\n",
    "            for x in response.json()['results']:\n",
    "                if x['formatted_address'].split()[-1].lower()==country.lower():\n",
    "                    i=i+1\n",
    "                    coordinate.append(x['geometry']['location']['lat'])\n",
    "                    coordinate.append(x['geometry']['location']['lng'])\n",
    "                else:\n",
    "                    continue\n",
    "            if i==0:\n",
    "                coordinate.append('Nan')\n",
    "        return coordinate\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the function to extract data from www.realtor.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_from_realtor_com(zipcode):\n",
    "    data = pd.DataFrame(columns = [\"address\",\"price/$\",\"bdrms\",\"baths\",\"link\",\"latitude\",\"longitude\",\"zipcode\"])\n",
    "    url=\"http://www.realtor.com/apartments/%s\"%(zipcode)\n",
    "    ori_url=\"http://www.realtor.com\"\n",
    "    response = requests.get(url)\n",
    "    page_data_soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    #to get the total_page_count to set the limit of page of the loop \n",
    "    try:\n",
    "        total_page_count = int(page_data_soup.find_all('span',class_=\"page\")[-1].text)\n",
    "    except:\n",
    "        total_page_count = 1\n",
    "        \n",
    "    #set page_count to go to the next page during the loop\n",
    "    page_count = 1\n",
    "    \n",
    "    #loop through all the pages\n",
    "    while page_count<=int(total_page_count):\n",
    "        url=\"http://www.realtor.com/apartments/%s/pg-%s\"%(zipcode,page_count)\n",
    "        response = requests.get(url)\n",
    "        page_data_soup = BeautifulSoup(response.content,'html.parser')\n",
    "        #get all the apts on one page\n",
    "        apt_each_page = page_data_soup.find_all(\"div\",class_=\"aspect-content\")\n",
    "        #loop through each apt on one page\n",
    "        for j in range(len(apt_each_page)):\n",
    "            single_result = page_data_soup.find_all('div',class_=\"aspect-content\")[j]\n",
    "            #to get the address\n",
    "            addr = single_result.find('span',class_=\"listing-street-address\").text\n",
    "            #get the link\n",
    "            addr_link = ori_url+single_result.find('a').get(\"href\")\n",
    "            #get the price\n",
    "            try:      \n",
    "                ori_price=single_result.find('div',class_=\"srp-item-price\").text.strip()\n",
    "                price = \"\".join(ori_price.split(','))\n",
    "            except:\n",
    "                price=\"Nan\"\n",
    "            #get the zipcode to make sure that apt found is indeed the zipcode we need\n",
    "            try:\n",
    "                zipcode = page_data_soup.find('span',class_=\"listing-postal\").text\n",
    "            except:\n",
    "                zipcode = 'Nan'\n",
    "            #sometimes one picture is with 2 apartment\n",
    "            #to get the number of bedroom\n",
    "            try:\n",
    "                see_if_mutli=len(single_result.find('li', {\"data-label\":\"property-meta-beds\"}).text)\n",
    "                bed=single_result.find('li', {\"data-label\":\"property-meta-beds\"}).text.strip()[0]\n",
    "            except:\n",
    "                bed = 'Nan'\n",
    "            #to get the number of bathroom\n",
    "            try:\n",
    "                bath=single_result.find('li', {\"data-label\":\"property-meta-baths\"}).text.strip()[0]  \n",
    "            except:\n",
    "                bath = 'Nan'\n",
    "            if see_if_mutli==4:\n",
    "                coordinate = get_lat_lng_json(addr,country='USA')\n",
    "                try:\n",
    "                    lat = get_lat_lng_json(addr,country='USA')[0]\n",
    "                    lng = get_lat_lng_json(addr,country='USA')[1]\n",
    "                except:\n",
    "                    lat = 'Nan'\n",
    "                    lng = 'Nan'\n",
    "                data.loc[data.shape[0]]=[addr,price,bed,bath,addr_link,lat,lng,zipcode]\n",
    "        page_count+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the function to extract data from www.zillow.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_from_zillow_com(zipcode):\n",
    "    data = pd.DataFrame(columns = [\"address\",\"price/$\",\"bdrms\",\"baths\",\"link\",\"latitude\",\"longitude\",\"zipcode\"])\n",
    "    url = \"http://www.zillow.com/homes/for_rent/Manhattan-New-York-NY-%s\"%(zipcode)\n",
    "    ori_url = \"http://www.zillow.com\"\n",
    "    response = requests.get(url)\n",
    "    page_data_soup = BeautifulSoup(response.content,'html.parser')\n",
    "    #to get the total_page_count\n",
    "    try:\n",
    "        total_page_count = int(page_data_soup.find(\"ol\",class_=\"zsg-pagination\").text.strip()[-5])\n",
    "    except:\n",
    "        total_page_count = 1\n",
    "    #print(total_page_count)\n",
    "    page_count = 1\n",
    "    #loop through all the pages\n",
    "    while page_count<=int(total_page_count):\n",
    "        url = \"http://www.zillow.com/homes/for_rent/Manhattan-New-York-NY-%s/%s_p/\"%(zipcode,page_count)\n",
    "        response = requests.get(url)\n",
    "        page_data_soup = BeautifulSoup(response.content,'html.parser')\n",
    "        #get all the apts on one page\n",
    "        apt_each_page = page_data_soup.find_all(\"div\",class_=\"zsg-photo-card-content zsg-aspect-ratio-content\")\n",
    "        #loop through each apt on one page\n",
    "        for j in range(len(apt_each_page)):\n",
    "            single_result = page_data_soup.find_all('div',class_=\"zsg-photo-card-content zsg-aspect-ratio-content\")[j]\n",
    "            try:\n",
    "                addr = single_result.find('span',class_=\"zsg-photo-card-address\").text\n",
    "                coordinate = get_lat_lng_json(addr,country='USA')\n",
    "                try:\n",
    "                    lat = get_lat_lng_json(addr,country='USA')[0]\n",
    "                    lng = get_lat_lng_json(addr,country='USA')[1]\n",
    "                except:\n",
    "                    lat = 'Nan'\n",
    "                    lng = 'Nan'\n",
    "                try:\n",
    "                    addr_link = ori_url+single_result.find('a').get(\"href\")\n",
    "                except:\n",
    "                    link = 'Nan'\n",
    "                try:\n",
    "                    ori_price = single_result.find('span',class_=\"zsg-photo-card-price\").text\n",
    "                    price = \"\".join(ori_price.split(','))\n",
    "                    try:\n",
    "                        bed = single_result.find('span',class_=\"zsg-photo-card-info\").text.strip()[0]\n",
    "                    except:\n",
    "                        bed ='Nan'\n",
    "                    try:\n",
    "                        bath_where = single_result.find('span',class_=\"zsg-photo-card-info\").text.strip()\n",
    "                        if bath_where[0] == str(1):\n",
    "                            bath = bath_where[7]\n",
    "                        else:\n",
    "                            bath = bath_where[8]\n",
    "                    except:\n",
    "                        bath = 'Nan'\n",
    "                except:\n",
    "                    for i in single_result.find_all('span',class_=\"zsg-photo-card-unit\"):\n",
    "                        ori_price = i.text.strip()[1:]\n",
    "                        price = \"\".join(ori_price.split(','))\n",
    "                        bed = i.text.strip()[0]\n",
    "                        bath = 'Nan'\n",
    "                        data.loc[data.shape[0]] = [addr,price,bed,bath,addr_link,lat,lng,zipcode]\n",
    "            except:\n",
    "                continue\n",
    "        page_count+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the function to extract data from www.rent.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_from_rent_com(zipcode):\n",
    "    data=pd.DataFrame(columns = [\"address\",\"price/$\",\"bdrms\",\"baths\",\"link\",\"latitude\",\"longitude\",\"zipcode\"])\n",
    "    url=\"http://www.rent.com/zip-%s\"%(zipcode)\n",
    "    response = requests.get(url)\n",
    "    page_data_soup = BeautifulSoup(response.content,'html.parser')\n",
    "    #to get the total listing amount\n",
    "    total_listings_count=page_data_soup.find('span',class_=\"total-listings-count\").text \n",
    "    page_count = 1\n",
    "    listings_count = 1\n",
    "    #loop through each page until we go through all the apts\n",
    "    while listings_count<=int(total_listings_count):\n",
    "        url = \"http://www.rent.com/zip-%s-houses/?page=%s\"%(zipcode,page_count)\n",
    "        response = requests.get(url)\n",
    "        page_data_soup = BeautifulSoup(response.content,'html.parser')\n",
    "        ori_url=\"http://www.rent.com\"\n",
    "        apt_each_page=page_data_soup.find_all('div',class_=\"prop-info result\")\n",
    "        for j in range(len(apt_each_page)):\n",
    "            single_result = page_data_soup.find_all('div',class_=\"prop-info result\")[j]\n",
    "            try:\n",
    "                add_where = single_result.find('div',class_=\"prop-title clearfix\")\n",
    "                addr = add_where.find('strong').text\n",
    "                coordinate = get_lat_lng_json(addr,country='USA')\n",
    "                try:\n",
    "                    lat = coordinate[0]\n",
    "                    lng = coordinate[1]  \n",
    "                except:\n",
    "                    lat = 'Nan'\n",
    "                    lng = 'Nan'\n",
    "                try:\n",
    "                    addr_link = ori_url+add_where.find('a').get(\"href\")\n",
    "                except:\n",
    "                    addr_link = 'Nan'\n",
    "                try:\n",
    "                    price_where = single_result.find('p',class_=\"prop-rent\")       \n",
    "                    price = price_where.find('span').text.strip()\n",
    "                except:\n",
    "                    price = 'Nan'\n",
    "                try:\n",
    "                    room_where = single_result.find('div',class_=\"prop-unit-info bullet-separator\")\n",
    "                except:\n",
    "                    bed = 'Nan'\n",
    "                    bath = 'Nan'\n",
    "                try:\n",
    "                    bed = room_where.find('span',class_=\"prop-beds bullet-separator\").text.strip()[0]\n",
    "                except:\n",
    "                    bed = 'Nan'\n",
    "                try:\n",
    "                    bath = room_where.find('span',class_=\"prop-baths bullet-separator\").text.strip()[0]\n",
    "                except:\n",
    "                    bath = 'Nan'\n",
    "                #write the info of each apts into the row of dataframe\n",
    "                data.loc[data.shape[0]] = [addr,price,bed,bath,addr_link,lat,lng,zipcode]\n",
    "            except:\n",
    "                continue\n",
    "            listings_count+= 1\n",
    "        #go to next page\n",
    "        page_count+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the function to extract data from www.trulia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_from_trulia(zipcode):\n",
    "    url = 'https://www.trulia.com/for_rent/{}_zip'.format(zipcode)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        page = BeautifulSoup(response.content,'html.parser')\n",
    "        p=[]\n",
    "    for i in range(100):\n",
    "        test = page.find('div',{\"class\":\"col cols16 mts txtC srpPagination_list\"})\n",
    "        if test is not None:\n",
    "            for all in test.find_all('a'):\n",
    "                p.append(all.text)\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    try:\n",
    "        max_page = int(p[-1])   \n",
    "    except:\n",
    "        max_page = 1\n",
    "    l= list()\n",
    "    for page in range(max_page):\n",
    "        url = 'https://www.trulia.com/for_rent/{}_zip/{}_p'.format(zipcode,page+1)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            page = BeautifulSoup(response.content,'lxml')\n",
    "            for tag in page.find_all('li',{'class':\"hover propertyCard property-data-elem clickable\"}):\n",
    "                #address\n",
    "                try:\n",
    "                    address = tag.find('span',{'itemprop':\"streetAddress\"}).get_text()\n",
    "                    coordinate = get_lat_lng_json(address,country='USA')\n",
    "                    #get the latitude and longitude\n",
    "                    try:\n",
    "                        lat = get_lat_lng_json(address,country='USA')[0]\n",
    "                        lng = get_lat_lng_json(address,country='USA')[1]                \n",
    "                    except:\n",
    "                        lat = 'Nan'\n",
    "                        lng = 'Nan'\n",
    "                    #get the price\n",
    "                    try:\n",
    "                        ori_price = tag.find('span',{'class':\"typeEmphasize\"}).get_text().strip()\n",
    "                        price = \"\".join(ori_price.split(','))\n",
    "                    except:\n",
    "                        price = 'Nan'\n",
    "                    #bathroom and bedroom\n",
    "                    b=[]\n",
    "                    try:\n",
    "                        for subtag in tag.find('div',{'class':'col cols3'}).find_all('div'):\n",
    "                            b.append(subtag.get_text().strip()[0])\n",
    "                    except:\n",
    "                        b[0] = 'Nan'\n",
    "                        b[1] = 'Nan'\n",
    "                    #link\n",
    "                    try:\n",
    "                        link = tag.find('a',{'class':'primaryLink pdpLink activeLink'}).get('href')\n",
    "                        link = 'https://www.trulia.com' + link\n",
    "                    except:\n",
    "                        link = 'Nan'\n",
    "                    l.append((address,price,b[0],b[1],link,lat,lng,zipcode))\n",
    "                except:\n",
    "                    continue\n",
    "        else:\n",
    "            print('We cannot find the website')\n",
    "    data=pd.DataFrame(l,columns=[\"address\",\"price/$\",\"bdrms\",\"baths\",\"link\",\"latitude\",\"longitude\",\"zipcode\"])\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the data seperately and combine them together\n",
    "the reason that we make them seperately is because sometimes if a website fails, we can still get results from other website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1_25 = extract_from_realtor_com(10025)\n",
    "d1_25.to_excel(\"d1_25.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2_25 = extract_from_zillow_com(10025)\n",
    "d2_25.to_excel(\"d2_25.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d3_25 = extract_from_rent_com(10025)\n",
    "d3_25.to_excel(\"d3_25.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d4_25 = extract_from_trulia(10025)\n",
    "d4_25.to_excel(\"d4_25.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1_26 = extract_from_realtor_com(10026)\n",
    "d1_26.to_excel(\"d1_26.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2_26 = extract_from_zillow_com(10026)\n",
    "d2_26.to_excel(\"d2_26.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d3_26 = extract_from_rent_com(10026)\n",
    "d3_26.to_excel(\"d3_26.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d4_26 = extract_from_trulia(10026)\n",
    "d4_26.to_excel(\"d4_26.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1_27 = extract_from_realtor_com(10027)\n",
    "d1_27.to_excel(\"d1_27.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2_27 = extract_from_zillow_com(10027)\n",
    "d2_27.to_excel(\"d2_27.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d3_27 = extract_from_rent_com(10027)\n",
    "d3_27.to_excel(\"d3_27.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d4_27 = extract_from_trulia(10027)\n",
    "d4_27.to_excel(\"d4_27.xlsx\",sheet_name='sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc_final = pd.concat([d1_25,d2_25,d3_25,d4_25,d1_26,d2_26,d3_26,d4_26,d1_27,d2_27,d3_27,d4_27],ignore_index=True)\n",
    "dc_final.to_excel('all_data.xlsx',sheet_name='Sheet 1')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
